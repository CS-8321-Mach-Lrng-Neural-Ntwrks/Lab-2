{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset: Stanford Sentiment Treebank\n",
    "pre train model: RoBERTa - roberta-base or ALBERT - albert-base-v2\n",
    "evaluation criteria: accuracy, f1-score, cross-entropy loss, precision & recall, and confusion matrix \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2.0 points] Give an overview of the dataset you have chosen to use.\n",
    "\n",
    "What is the classification task? What business case does it solve? Is this multi-task? Explain.\n",
    "What is the feature data? Who collected the data? Why? When? Is the data multi-modal?\n",
    "What evaluation criteria will you be using and why? Why does this support the business case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we want to use is the Stanford Sentiment Treebank form link. (https://research.aimultiple.com/sentiment-analysis-dataset/#stanford-sentiment-treebank, https://nlp.stanford.edu/sentiment/index.html ) This is a many to one dataset and this dataset contains about 10,000 reviews on the movies with a sentiment score from 1 to 25. The most positive is 25 and the most negative is 1. This dataset is used to classify the sentiment of movie review into positive, negative, or neutral categories based on the sentiment score.\n",
    "\n",
    "\n",
    "The business cases that this dataset can solve are analyzing the customerâ€™s feedback, monitoring the social media reaction after the movie release, etc. For the film industry, they can use the model trained with this dataset to monitor the reviews of their released movies on social media and adjust their marketing strategies in a timely manner, or movie rating platforms such as RottenTomatoes can have more optimized rating strategies.\n",
    "\n",
    "\n",
    "This is a single task classification, the main goal of this dataset is to predict the sentiment of a given sentence. In the dataset multiple words are sequences that contribute to a single sentiment label 1 to 25. \n",
    "\n",
    "\n",
    "The feature data of this dataset are the text data collection from the movie reviews. The labels of this dataset are the sentiment scores from 1 to 25. In this dataset each phrase is assigned a sentiment score. This dataset was collected by the researchers form stanford university in 2013. The reason why they collect this dataset is because the traditional sentiment analysis models classify the full sentence, but human language is usually ambiguous. If we continue to distinguish it based on a whole sentence, we will ignore the influence of some words. For example, if a movie review is \"not bad\" or \"not too bad\", the traditional model will ignore the influence of the preceding \"not\" or \"not too\" and directly classify the review as bad, a negative score. In fact, \"not bad\" is more like a neutral score. That is why they collect this dataset and provide phrase level sentiment labels, and they use the tree based structure to capture the compositional sentiment. They think this can help the machine learning model have a better understanding in the complex sentence structure. \n",
    "\n",
    "\n",
    "This data is not a multi-model, it is unimodal. Because only text data in this data set. \n",
    "\n",
    "\n",
    "This dataset can help solve the business case like analyzing customer feedback and monitoring social media reaction. So we want to use accuracy, f1-score, cross-entropy loss, precision & recall, and confusion matrix as the evaluation criteria. Here are the reasons: the accuracy used to measure the proportion of correctly classified reviews, this for the general measurement for the model. The F1 Score is the balance of precision and recall. For the sentiment classification tasks, to eliminate the effect of imbalanced data, the f1 score is more informative than accuracy. The cross-entropy loss helps optimize the model confidence in the correct classifications. Is used for measuring the divergence between predicted probability distribution and actual labels. The precision & recall can ensure the positive sentiment predictions are truly positive and also ensure the model identifies the relevant sentiment example as possible. The confuse matrix provides the insight of misclassification patterns, which can help as to know which sentiment classes are confused with each other. \n",
    "\n",
    "\n",
    "For our business case like monitor the social media reaction or movies review rating. A high f1 score can ensure the model correctly classifies the sentiment. The precision & recall help as avoiding the false signal when tracking audience sentiment across social media. The improvement on the accuracy cna help the review platforms reducing the bias in their score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2.0 points] Describe the foundational model that you will be using to transfer learn from. What task(s) was this foundational model trained upon? Explain if the new task is within the same domain, across domains, etc. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The foundational model we want to use to transfer learning form is ALBERT-Base-v2. This model is a variant to BERT that can improve the efficiency while keeping the performance at the same level. This model was introduced by Google research in 2019, and is design yo solve the limitation of BERT when performing the task like parameter reduction techniques. The albert-base-v2 model has 12 repeating layers, 128 embedding, 768-hidden, 12-heads, 11M parameters. https://huggingface.co/transformers/v3.3.1/pretrained_models.html, https://github.com/google-research/ALBERT The ALBERT was trained using for self-supervised learning on the large task corpora. It was pre trained to perform the task like: masked language modeling, which is the model randomly masks some words, makes it invisible and learns to predict the missing words. Also trained for sentence order prediction, it was trained to detect if the two sentences were swapped with each other. The new task sentiment classification is within the same domain as the pre-training tasks, because both tasks need the model to understand the structure of the sentences. The ALBERT already understands the contextual word relationships, and phrase structure, so we think is can be fine tuned efficiently for the sentiment classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1.0 points] Split the data into training and testing. Be sure to explain how you performed this operation and why you think it is reasonable to split this particular dataset this way. For multi-task datasets, be sure to explain if it is appropriate to stratify within each task. If the dataset is already split for you, explain how the split was achieved and how it is stratified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total extracted phrases: 41447\n",
      "\n",
      "Sentiment score distribution:\n",
      "sentiment_score\n",
      "0     1070\n",
      "1     4613\n",
      "2    28305\n",
      "3     5781\n",
      "4     1678\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset saved to './Dataset/parsed_dev_dataset.csv'\n",
      "\n",
      "Sample phrases by sentiment score:\n",
      "\n",
      "Score 0:\n",
      "  - \"Sticky sweet sentimentality , clumsy plotting and a rosily myopic view of life in the WWII-era Mississippi Delta undermine this adaptation .\"\n",
      "  - \"drab and sordid\"\n",
      "  - \"dull\"\n",
      "\n",
      "Score 1:\n",
      "  - \"could not find a buyer to play it on the tube\"\n",
      "  - \"ridiculous\"\n",
      "  - \"we are left with something like two ships passing in the night rather than any insights into gay love ,\"\n",
      "\n",
      "Score 2:\n",
      "  - \"but\"\n",
      "  - \"to\"\n",
      "  - \"studio horror franchise\"\n",
      "\n",
      "Score 3:\n",
      "  - \"quality\"\n",
      "  - \"the most emotional resonance\"\n",
      "  - \"Martin and Barbara are complex characters -- sometimes tender , sometimes angry --\"\n",
      "\n",
      "Score 4:\n",
      "  - \"delectable and intriguing thriller\"\n",
      "  - \"riveting , pulse intensifying escapist adventure\"\n",
      "  - \"Feature debuter D.J. Caruso directs a crack ensemble cast , bringing screenwriter Tony Gayton 's narcotics noir to life .\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tree import Tree\n",
    "\n",
    "# Function to parse the tree structure and extract phrases with sentiment scores\n",
    "def parse_trees_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read().strip()\n",
    "    \n",
    "    # Split into individual trees (sentences)\n",
    "    tree_strings = data.split('\\n')\n",
    "    \n",
    "    all_phrases = []\n",
    "    \n",
    "    for tree_str in tree_strings:\n",
    "        try:\n",
    "            # Parse the tree\n",
    "            tree = Tree.fromstring(tree_str)\n",
    "            \n",
    "            # Extract phrases and scores\n",
    "            phrases_with_scores = extract_phrases_and_scores(tree)\n",
    "            all_phrases.extend(phrases_with_scores)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing tree: {e}\")\n",
    "            continue\n",
    "    \n",
    "    df = pd.DataFrame(all_phrases, columns=['phrase', 'sentiment_score'])\n",
    "    return df\n",
    "\n",
    "# Function to recursively extract phrases and their sentiment scores\n",
    "def extract_phrases_and_scores(tree):\n",
    "    if not isinstance(tree, Tree):\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Get the label\n",
    "    label = int(tree.label())\n",
    "    \n",
    "    # Get the phrase\n",
    "    phrase = ' '.join(tree.leaves())\n",
    "    \n",
    "    # Add phrase and its score\n",
    "    if phrase:\n",
    "        results.append((phrase, label))\n",
    "    \n",
    "    # process each subtree\n",
    "    for subtree in tree:\n",
    "        if isinstance(subtree, Tree):\n",
    "            results.extend(extract_phrases_and_scores(subtree))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Parse the dev.txt file\n",
    "dataset_path = './Dataset/dev.txt'\n",
    "dev_df = parse_trees_from_file(dataset_path)\n",
    "\n",
    "# Display statistics\n",
    "print(f\"Total extracted phrases: {len(dev_df)}\")\n",
    "print(\"\\nSentiment score distribution:\")\n",
    "print(dev_df['sentiment_score'].value_counts().sort_index())\n",
    "\n",
    "# Save the dataset\n",
    "dev_df.to_csv('./Dataset/parsed_dev_dataset.csv', index=False)\n",
    "print(\"\\nDataset saved to './Dataset/parsed_dev_dataset.csv'\")\n",
    "\n",
    "# Sample of phrases with different sentiment scores\n",
    "print(\"\\nSample phrases by sentiment score:\")\n",
    "for score in range(5):  # 0 to 4\n",
    "    sample = dev_df[dev_df['sentiment_score'] == score].sample(min(3, len(dev_df[dev_df['sentiment_score'] == score])))\n",
    "    print(f\"\\nScore {score}:\")\n",
    "    for phrase in sample['phrase']:\n",
    "        print(f\"  - \\\"{phrase}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train-Test Split for the Stanford Sentiment Treebank Dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Train-Test Split for the Stanford Sentiment Treebank Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset if not already loaded\n",
    "try:\n",
    "    dev_df\n",
    "except NameError:\n",
    "    dev_df = pd.read_csv('./Dataset/parsed_dev_dataset.csv')\n",
    "\n",
    "# First, let's analyze the dataset characteristics\n",
    "print(f\"Total number of phrases: {len(dev_df)}\")\n",
    "print(\"\\nSentiment distribution:\")\n",
    "sentiment_counts = dev_df['sentiment_score'].value_counts().sort_index()\n",
    "print(sentiment_counts)\n",
    "\n",
    "# Visualize the sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values)\n",
    "plt.title('Distribution of Sentiment Classes in Stanford Sentiment Treebank')\n",
    "plt.xlabel('Sentiment Score (0=Very Negative, 4=Very Positive)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(range(5), ['Very Negative', 'Negative', 'Neutral', 'Positive', 'Very Positive'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze phrase length distribution\n",
    "dev_df['phrase_length'] = dev_df['phrase'].apply(lambda x: len(x.split()))\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=dev_df, x='phrase_length', hue='sentiment_score', bins=20, multiple='stack')\n",
    "plt.title('Distribution of Phrase Lengths by Sentiment Score')\n",
    "plt.xlabel('Number of Words in Phrase')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(0, 30)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Train-Test Split with Stratification\n",
    "X = dev_df['phrase']\n",
    "y = dev_df['sentiment_score']\n",
    "\n",
    "# Perform stratified split to maintain sentiment distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {len(X_train)} phrases\")\n",
    "print(f\"Test set size: {len(X_test)} phrases\")\n",
    "\n",
    "# Verify the split maintains the sentiment distribution\n",
    "train_sentiment_counts = pd.Series(y_train).value_counts(normalize=True).sort_index() * 100\n",
    "test_sentiment_counts = pd.Series(y_test).value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "print(\"\\nSentiment distribution (%) in training set:\")\n",
    "print(train_sentiment_counts)\n",
    "print(\"\\nSentiment distribution (%) in test set:\")\n",
    "print(test_sentiment_counts)\n",
    "\n",
    "# Compare the distributions visually\n",
    "plt.figure(figsize=(12, 6))\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Training Set': train_sentiment_counts,\n",
    "    'Test Set': test_sentiment_counts\n",
    "})\n",
    "comparison_df.plot(kind='bar')\n",
    "plt.title('Sentiment Distribution Comparison Between Training and Test Sets')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.xticks(range(5), ['Very Negative', 'Negative', 'Neutral', 'Positive', 'Very Positive'], rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the split datasets for future use\n",
    "train_df = pd.DataFrame({'phrase': X_train, 'sentiment_score': y_train}).reset_index(drop=True)\n",
    "test_df = pd.DataFrame({'phrase': X_test, 'sentiment_score': y_test}).reset_index(drop=True)\n",
    "\n",
    "train_df.to_csv('./Dataset/sst_train.csv', index=False)\n",
    "test_df.to_csv('./Dataset/sst_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yolov5_env)",
   "language": "python",
   "name": "yolov5_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
